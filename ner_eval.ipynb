{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10797378,"sourceType":"datasetVersion","datasetId":6701287}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install transformers\n!pip install ujson","metadata":{"id":"D5WNLMCkLCk1"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"%%capture\nfrom transformers import pipeline\nimport logging\nimport os\nimport ujson\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nimport requests\nfrom bs4 import BeautifulSoup","metadata":{"id":"l2w8cB-C3zmW"},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def get_entity_name(entity_id):\n    # URL per ottenere i dettagli dell'entità\n    entity_url = f\"https://www.wikidata.org/wiki/Special:EntityData/{entity_id}.json\"\n\n    response = requests.get(entity_url)\n    if response.status_code != 200:\n        raise Exception(f\"Errore durante l'accesso ai dettagli dell'entità: {response.status_code}\")\n\n    entity_data = response.json()\n\n    # Estrarre i dettagli dell'entità\n    entity_labels = entity_data[\"entities\"][next(iter(entity_data[\"entities\"]))][\"labels\"]\n    # Ottenere il nome dell'entità in inglese\n    entity_name = entity_labels.get(\"en\", {}).get(\"value\")\n\n    return entity_name","metadata":{"id":"69pNuS-xPKgw"},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def get_entity_id(entity_name):\n    # URL di ricerca sul sito Wikidata\n    search_url = f\"https://www.wikidata.org/w/index.php\"\n    params = {\n        \"search\": entity_name,\n        \"title\": \"Special:Search\",\n        \"profile\": \"advanced\",  # Usa il profilo avanzato per i risultati più accurati\n        \"fulltext\": 1,\n        \"ns0\": 1,  # Cerca solo nello spazio principale (entità)\n    }\n\n    # Richiesta al sito Wikidata\n    response = requests.get(search_url, params=params)\n    if response.status_code != 200:\n        raise Exception(f\"Errore durante l'accesso alla pagina di ricerca: {response.status_code}\")\n\n    # Analisi del contenuto HTML\n    soup = BeautifulSoup(response.text, \"html.parser\")\n\n    # Trova il primo risultato\n    first_result = soup.find(\"div\", class_=\"mw-search-result-heading\")\n    if first_result:\n        # Estrai il link associato al risultato\n        link = first_result.find(\"a\")[\"href\"]\n        # Estrai il codice dell'entità dal link\n        entity_id = link.split(\"/\")[-1]\n\n        # URL per ottenere i dettagli dell'entità\n        entity_url = f\"https://www.wikidata.org/wiki/Special:EntityData/{entity_id}.json\"\n\n        response = requests.get(entity_url)\n        if response.status_code != 200:\n            raise Exception(f\"Errore durante l'accesso ai dettagli dell'entità: {response.status_code}\")\n\n        return list(response.json()['entities'].keys())[0]\n\n    else:\n        # print(\"Nessun risultato trovato.\")\n        return None","metadata":{"id":"FxG_-vJa0o3a"},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def clean_entities(entities):\n    results = []\n    for entity in entities:\n        # Clean up extra spaces introduced during tokenization\n        cleaned_word = entity['word'].replace(\" ' \", \"' \")\n        results.append(cleaned_word)\n    return results","metadata":{"id":"PQheflEgO2LM"},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def find_entity(sentence):\n  entities = ner_pipeline(sentence)\n  cleaned_words = clean_entities(entities)\n  return combine_words(cleaned_words)","metadata":{"id":"RoWKM9gvWFSO"},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def combine_words(words):\n    combined = []\n    temp = \"\"\n\n    for word in words:\n        # Se il token inizia con '##', è una parte di una parola più lunga\n        if word.startswith('##'):\n            temp += word[2:]  # Aggiungi solo la parte dopo '##'\n        else:\n            if temp:  # Se c'era una parte precedentemente accumulata, aggiungila\n                combined.append(temp)\n            temp = word  # Inizia una nuova parola\n    if temp:\n        combined.append(temp)  # Aggiungi l'ultima parte accumulata\n\n    return \" \".join(combined)","metadata":{"id":"IST93Cduun9v"},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def process_line(line):\n    data = ujson.loads(line)\n    sentence = data.get('source')\n    entities_id = data.get('wikidata_id')\n\n    tot = 1\n    generated = 0\n    corrects = 0\n\n    true_entity = get_entity_name(entities_id)\n    entitiy_obtained = find_entity(sentence)\n\n    true_id = get_entity_id(true_entity)\n    id_obtained = get_entity_id(entitiy_obtained)\n\n    if id_obtained:\n      generated = 1\n      if id_obtained == true_id:\n        corrects = 1\n\n    # print(tot,corrects, generated)\n    return tot, corrects, generated","metadata":{"id":"pHxQ7D6ShtIx"},"outputs":[],"execution_count":45},{"cell_type":"code","source":"def evaluate_NER(file_path,n_lines):\n  with open(file_path, 'r') as file:\n    limited_lines = (line for i, line in enumerate(file) if i < n_lines)\n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(process_line, limited_lines), desc=\"\\tProcessing lines\"))\n\n    tot = sum(res[0] for res in results)\n    corrects = sum(res[1] for res in results)\n    generated = sum(res[2] for res in results)\n\n    return tot,corrects,generated","metadata":{"id":"V0s0o1zhWAvY"},"outputs":[],"execution_count":40},{"cell_type":"code","source":"folder_path = \"semeval-data/references/validation\"\nresults= dict()\nner_names = [\n    \"dslim/bert-base-NER\",\n    \"dslim/bert-large-NER\",\n    \"Jean-Baptiste/camembert-ner\"\n]\n\nfor ner_name in ner_names:\n  print(\"=====================================================\")\n  print('NER:',ner_name)\n  print(\"=====================================================\")\n  tot,corrects,generated= 0,0,0\n  for file_name in os.listdir(folder_path):\n\n      file_path = os.path.join(folder_path, file_name)\n      if os.path.isfile(file_path):\n          print('\\tanalyzing',file_name)\n          # Set logging level to suppress the warning\n          logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n          ner_pipeline = pipeline(model=ner_name, aggregation_strategy='simple', device=0)\n          res = evaluate_NER(file_path, n_lines=50)\n          tot += res[0]\n          corrects += res[1]\n          generated += res[2]\n\n  precision = corrects / generated if generated > 0 else 0\n  recall = corrects / tot if tot > 0 else 0\n  f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n  results[ner_name]= {'\\tprecision':precision, 'recall': recall, 'f1_score': f1_score}","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6KU3OXqeSmPB","outputId":"52fdc313-973d-4b33-c3a4-c4b27403060e"},"outputs":[{"output_type":"stream","name":"stdout","text":["=====================================================\n","NER: dslim/bert-base-NER\n","=====================================================\n","\tanalyzing zh_TW.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:24,  2.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing ar_AE.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:27,  1.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing th_TH.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:28,  1.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing es_ES.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:23,  2.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing de_DE.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:27,  1.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing tr_TR.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:25,  1.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing it_IT.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:25,  1.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing fr_FR.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:24,  2.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing ko_KR.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:28,  1.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing ja_JP.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:29,  1.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["=====================================================\n","NER: dslim/bert-large-NER\n","=====================================================\n","\tanalyzing zh_TW.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:27,  1.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing ar_AE.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:30,  1.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing th_TH.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:28,  1.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing es_ES.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:23,  2.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing de_DE.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:26,  1.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing tr_TR.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:25,  1.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing it_IT.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:25,  1.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing fr_FR.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:24,  2.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing ko_KR.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:25,  1.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing ja_JP.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:30,  1.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["=====================================================\n","NER: Jean-Baptiste/camembert-ner\n","=====================================================\n","\tanalyzing zh_TW.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:34,  1.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing ar_AE.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:32,  1.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing th_TH.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:30,  1.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing es_ES.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:27,  1.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing de_DE.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:29,  1.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing tr_TR.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:29,  1.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing it_IT.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:25,  1.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing fr_FR.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:25,  1.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing ko_KR.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:26,  1.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\tanalyzing ja_JP.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\tProcessing lines: 50it [00:27,  1.83it/s]\n"]}],"execution_count":46},{"cell_type":"code","source":"for result in results.items():\n  print(result[0])\n  print(result[1])\n  print()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j79kC6A6s7su","outputId":"1fded25a-1721-4271-a81a-bcf58e85c3c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["dslim/bert-base-NER\n","{'\\tprecision': 0.7572815533980582, 'recall': 0.624, 'f1_score': 0.6842105263157895}\n","\n","dslim/bert-large-NER\n","{'\\tprecision': 0.7598039215686274, 'recall': 0.62, 'f1_score': 0.6828193832599119}\n","\n","Jean-Baptiste/camembert-ner\n","{'\\tprecision': 0.834070796460177, 'recall': 0.754, 'f1_score': 0.792016806722689}\n","\n"]}],"execution_count":47}]}